{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "\n",
    "# dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "# dataset_name=\"lecture_dataset_train\", \n",
    "# persistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because I have the dataset saved locally, I will load it like so\n",
    "cloned_dataset = fo.load_dataset(\"lecture_dataset_train_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #clone the dataset to avoid modifying the original dataset\n",
    "# cloned_dataset = dataset.clone(name=\"lecture_dataset_train_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image complexity\n",
    "\n",
    "We can use Canny edge detection to measure the ratio of edge pixels to total pixels\n",
    "\n",
    "This metric can be useful because:\n",
    "\n",
    "1. It provides a measure of the level of detail and intricacy in an image.\n",
    "\n",
    "2. Higher complexity can indicate more challenging images for object detection.\n",
    "\n",
    "3. It can help identify images that might require more processing power or sophisticated algorithms for accurate analysis.\n",
    "\n",
    "4. Understanding image complexity can aid in balancing datasets and evaluating model performance across different complexity levels.\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- **Oversimplification:** Edge detection reduces an image to binary information (edge or non-edge), discarding valuable texture and color information that could be crucial for object detection.\n",
    "\n",
    "- **Sensitivity to Noise:** Canny edge detection can be sensitive to image noise, potentially leading to inaccurate complexity assessments in noisy images.\n",
    "\n",
    "- **Parameter Dependency:** The effectiveness of Canny edge detection heavily relies on the chosen threshold parameters (100 and 200 in this case), which may not be optimal for all images in a diverse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_complexity(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the complexity of images in a FiftyOne dataset using Canny edge detection and color information.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (fiftyone.core.dataset.Dataset): FiftyOne dataset object.\n",
    "\n",
    "    Returns:\n",
    "    None. It just adds the field to the dataset\n",
    "    \"\"\"\n",
    "    for sample in dataset.iter_samples():\n",
    "        img = cv2.imread(sample.filepath)\n",
    "        # Convert the image to float32\n",
    "        img_float = img.astype(np.float32) / 255.0\n",
    "        # Calculate the color variance for the image\n",
    "        color_variance = np.var(img_float, axis=(0, 1)).sum()\n",
    "        # Convert to grayscale for edge detection\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        edge_complexity = np.sum(edges > 0) / (img.shape[0] * img.shape[1])\n",
    "        # Combine edge complexity and color variance\n",
    "        complexity = edge_complexity + color_variance\n",
    "        sample[\"image_complexity_score\"] = complexity\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_image_complexity(cloned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual clutter \n",
    "\n",
    "Calculates the variance of pixel intensities in the image.\n",
    "\n",
    "This metric is useful because:\n",
    "\n",
    "1. It measures the level of disorder or chaos in an image, which can impact object detection.\n",
    "\n",
    "2. High visual clutter can make it more difficult to isolate and identify individual objects.\n",
    "\n",
    "3. It provides insight into the visual complexity of scenes beyond just object count or density.\n",
    "\n",
    "4. Understanding visual clutter can help in developing strategies to improve model performance on visually complex images.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- **Oversimplification:** Reducing visual clutter to a single variance value may oversimplify the concept, missing important spatial relationships.\n",
    "\n",
    "- **Grayscale Conversion:** Converting to grayscale loses color information, which can be a significant factor in visual clutter and object detection.\n",
    "\n",
    "- **Global Measure:** The global variance doesn't capture local variations in clutter, which might be more relevant for object detection.\n",
    "\n",
    "- **Insensitivity to Structure:** High variance doesn't necessarily correlate with difficulty in object detection. A highly structured image could have high variance but be relatively easy for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_visual_clutter(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the visual clutter of images in a FiftyOne dataset using pixel intensity variance and color variance.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (fiftyone.core.dataset.Dataset): FiftyOne dataset object.\n",
    "\n",
    "    Returns:\n",
    "    None. It just adds the field to the dataset.\n",
    "    \"\"\"\n",
    "    for sample in dataset.iter_samples():\n",
    "        img = cv2.imread(sample.filepath)\n",
    "        \n",
    "        # Calculate grayscale variance\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray_clutter = np.var(gray)\n",
    "        \n",
    "        # Calculate color variance\n",
    "        img_float = img.astype(np.float32) / 255.0  # Convert to float32\n",
    "        color_variance = np.var(img_float, axis=(0, 1)).sum()\n",
    "        \n",
    "        # Combine both measures\n",
    "        clutter = gray_clutter + color_variance\n",
    "        sample[\"visual_clutter_score\"] = clutter\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_visual_clutter(cloned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Clutter\n",
    "\n",
    "An object vlutter score will identify number of detections per image. This is a simple and useful metric. It provides a quick measure of how busy or crowded an image is in terms of objects.\n",
    "\n",
    "This metric is useful because:\n",
    "\n",
    "1. It provides a simple measure of scene complexity in terms of object count.\n",
    "\n",
    "2. Higher clutter scores can indicate more challenging images for object detection.\n",
    "\n",
    "3. It helps identify images that may require more processing time or have higher chances of false positives/negatives.\n",
    "\n",
    "4. Understanding clutter can aid in balancing datasets and evaluating model performance across different complexity levels.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Easy to calculate and interpret\n",
    "\n",
    "- Gives a clear indication of image complexity\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Doesn't account for object size or distribution\n",
    "\n",
    "- May not distinguish between genuinely cluttered scenes and scenes with many small objects\n",
    "\n",
    "**Usefulness:** High, especially as a basic measure of image complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "detection_counts = cloned_dataset.values(F(\"ground_truth.detections\").length())\n",
    "\n",
    "cloned_dataset.set_values(\"object_clutter_score\", detection_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object diversity\n",
    "\n",
    "This will measure the number of distinct classes per image. This is an excellent metric for measuring the semantic diversity of an image.\n",
    "\n",
    "This metric is useful because:\n",
    "\n",
    "1. It quantifies the variety of object types present in an image.\n",
    "\n",
    "2. Higher diversity can indicate more complex scenes that require broader object recognition capabilities.\n",
    "\n",
    "3. It helps in assessing the range of objects a model needs to handle within a single image.\n",
    "\n",
    "4. Understanding instance diversity can guide dataset curation to ensure a wide range of object combinations are represented.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Directly measures the variety of object types in an image\n",
    "\n",
    "- Easy to calculate and interpret\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Doesn't account for the number of instances of each class\n",
    "\n",
    "- Treats all classes equally, regardless of their visual or semantic similarity\n",
    "\n",
    "**Usefulness:** High, particularly for understanding the range of objects a model needs to handle.\n",
    "\n",
    "---\n",
    "Note: Below we're using `$` in the ViewField. The \"$\" in FiftyOne is like an \"absolute path\" in file systems.\n",
    "\n",
    "- Without \"$\": Relative path. Depends on where you are in the data structure.\n",
    "\n",
    "- With \"$\": Absolute path. Always starts from the root of each sample.\n",
    "\n",
    "Use \"$\" when:\n",
    "\n",
    "1. In complex queries where context might be ambiguous\n",
    "2. Inside operations like map() or reduce()\n",
    "3. You want to be explicitly clear you're referring to a top-level field\n",
    "\n",
    "You often don't need \"$\" for simple, top-level queries.\n",
    "\n",
    "In this case, `F(\"$ground_truth.detections.label\")` ensures you're accessing the correct data structure (the top-level `ground_truth` field), and prevents FiftyOne from mistakenly operating on a string instead of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "object_diversity_expression = F(\"$ground_truth.detections.label\").unique().length()\n",
    "\n",
    "object_diversity_scores = cloned_dataset.values(object_diversity_expression)\n",
    "\n",
    "cloned_dataset.set_values(\"object_diversity_score\", object_diversity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Diversity Ratio\n",
    "\n",
    "This metric considers the number of detections and number of classes per image. This is a more nuanced approach to measuring diversity that takes into account both the number of objects and the variety of classes.\n",
    "\n",
    "This metric is useful because:\n",
    "\n",
    "1. It balances the number of objects with the variety of object types, providing a more nuanced view of image complexity.\n",
    "\n",
    "2. It can distinguish between images with many objects of few classes and those with fewer objects but more diverse classes.\n",
    "\n",
    "3. Higher ratios might indicate images that could present challenges to an object detection system.\n",
    "\n",
    "4. This metric can help in creating balanced datasets that challenge models in different ways.\n",
    "\n",
    "I'm using `log10(object_clutter) + 1` in the denominator to put more emphasis on the number of unique classes relative to the total number of detections. This aim is to highlight images with a diverse range of object classes even if they don't have an extremely high number of total detections.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Combines quantity and variety of objects\n",
    "\n",
    "- Can distinguish between images with many objects of few classes and those with fewer objects but more classes\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- May require careful design to balance the influence of object count and class count\n",
    "\n",
    "- Interpretation might be less intuitive than simpler metrics\n",
    "\n",
    "**Usefulness:** High, as it provides a more comprehensive view of image complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "diversity_expression = F(\"object_diversity_score\") / (F(\"object_clutter_score\").log10() + 1)\n",
    "\n",
    "diversity_ratios = cloned_dataset.values(diversity_expression)\n",
    "\n",
    "cloned_dataset.set_values(\"diversity_ratio\", diversity_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectness score \n",
    "\n",
    "This metric measures the percentage of pixels that belong to classes across the whole image. Basically what percentage of the pixels contains a detection.\n",
    "\n",
    "1. It provides insight into how much of the image is occupied by objects of interest.\n",
    "\n",
    "2. Lower scores might indicate images with large background areas or small objects, which can be challenging for detection.\n",
    "\n",
    "3. It can help identify images where objects occupy a significant portion of the scene, potentially affecting detection strategies.\n",
    "\n",
    "4. Understanding objectness can aid in analyzing model performance relative to object size and prominence in the image.\n",
    "\n",
    "This is a valuable metric for understanding how much of the image is occupied by objects of interest.\n",
    "\n",
    "**Pros:**\n",
    "- Provides insight into the density of annotated objects\n",
    "- Can help identify images with large background areas\n",
    "\n",
    "**Cons:**\n",
    "- Doesn't account for the number or diversity of objects\n",
    "- May be biased towards images with large objects\n",
    "\n",
    "**Usefulness:** High, especially when combined with other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "cloned_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()\n",
    "\n",
    "cloned_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectness_scores = cloned_dataset.values(F(\"$ground_truth.detections.relative_bbox_area\").sum())\n",
    "\n",
    "cloned_dataset.set_values(\"objectness_score\", objectness_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Distribution Score\n",
    "This metric is useful because:\n",
    "1. It quantifies how spread out or clustered objects are within an image.\n",
    "2. Images with more evenly distributed objects might present different challenges than those with clustered objects.\n",
    "3. It can help identify images where objects are tightly grouped, which might be challenging for object separation.\n",
    "4. Understanding spatial distribution can aid in developing models that perform well across various object arrangements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box_centroid = (\n",
    "    (F(\"bounding_box\")[0] + F(\"bounding_box\")[2]/2),\n",
    "    (F(\"bounding_box\")[1] + F(\"bounding_box\")[3]/2)\n",
    "    )\n",
    "\n",
    "cloned_dataset.set_field(\"ground_truth.detections.bbox_centroid\", bounding_box_centroid).save()\n",
    "\n",
    "centroids = cloned_dataset.values(\"ground_truth.detections.bbox_centroid\")\n",
    "\n",
    "averaege_pair_wise_distances = [np.mean(pdist(c)) if len(c) > 0 else 0 for c in centroids]\n",
    "\n",
    "cloned_dataset.set_values(\"spatial_distribution_score\", averaege_pair_wise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(cloned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using zero shot models to get scene info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Make zero-shot predictions with custom classes\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"\",\n",
    "    classes=[],\n",
    ")\n",
    "\n",
    "cloned_dataset.apply_model(model, label_field=\"scene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required reading:\n",
    "\n",
    "https://docs.voxel51.com/recipes/creating_views.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
