{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "# import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# train_dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "# dataset_name=\"lecture_dataset_train\", \n",
    "# persistent=True)\n",
    "\n",
    "\n",
    "# test_dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_test\", \n",
    "# dataset_name=\"lecture_dataset_test\", \n",
    "# persistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because I have the dataset saved locally, I will load it like so\n",
    "train_dataset = fo.load_dataset(\"lecture_dataset_train\")\n",
    "\n",
    "# and just cloning it as I don't want to modify the original\n",
    "train_dataset = train_dataset.clone(name=\"lecture-train-clone\")\n",
    "\n",
    "test_dataset = fo.load_dataset(name=\"lecture_dataset_test\")\n",
    "test_dataset = test_dataset.clone(name=\"lecture-test-clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Analysis\n",
    "\n",
    "Note: that FiftyOne Detections are relative bounding box coordinates in [0, 1] in the following format:`[top-left-x, top-left-y, width, height]`\n",
    "\n",
    "Let's break down the next code block intuitively:\n",
    "\n",
    "* `rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]`\n",
    "   This calculates the relative area of a bounding box. In FiftyOne, bounding boxes are typically stored as `[x, y, width, height]` where x and y are the coordinates of the top-left corner, and width and height are relative to the image size (values between 0 and 1). So this line multiplies the relative width and height to get the relative area.\n",
    "\n",
    "* `im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")`\n",
    "   This retrieves the actual width and height of each image from the metadata. The \"$\" indicates that we're accessing a field at the sample level, not within a detection.\n",
    "\n",
    "* `abs_area = rel_bbox_area * im_width * im_height`\n",
    "   This calculates the absolute area of the bounding box in pixels. It does this by multiplying the relative area by the image dimensions.\n",
    "\n",
    "* `train_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()`\n",
    "   This adds a new field to each detection in the ground truth, storing the relative area of the bounding box.\n",
    "\n",
    "* `train_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()`\n",
    "   This adds another new field to each detection, this time storing the absolute area of the bounding box in pixels.\n",
    "\n",
    "In essence, this code is adding two new pieces of information to each detection in our dataset:\n",
    "\n",
    "1. The relative area of the bounding box (as a fraction of the total image area)\n",
    "\n",
    "2. The absolute area of the bounding box (in pixels)\n",
    "\n",
    "These new fields can be very useful for further analysis, such as filtering detections based on their size, or analyzing the distribution of object sizes in your dataset. The beauty of using `ViewField` (`F`) is that these calculations are done efficiently across the entire dataset without needing to iterate through each sample manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "train_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()\n",
    "\n",
    "train_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.first().ground_truth.detections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the upper and lower [bounds](https://docs.voxel51.com/api/fiftyone.core.aggregations.html#fiftyone.core.aggregations.Bounds) of the bounding box areas as well as other summary statistics like mean and standard deviation. Note: these are relative bounding box areas, so they represent the percentage of the total image area (hence the multiply by 100).\n",
    "\n",
    "Here's how you can do that:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = train_dataset.filter_labels(\"ground_truth\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"ground_truth.detections.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"ground_truth.detections.relative_bbox_area\")*100\n",
    "    std = view.std(\"ground_truth.detections.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of bounding box sizes across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get all labels and relative bbox areas\n",
    "labels = train_dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "bbox_areas = train_dataset.values(\"ground_truth.detections.absolute_bbox_area\")\n",
    "\n",
    "# Flatten the lists and create the dataframe\n",
    "data = []\n",
    "for sample_labels, sample_areas in zip(labels, bbox_areas):\n",
    "    for label, area in zip(sample_labels, sample_areas):\n",
    "        data.append({'label': label, 'absolute_bbox_area': area})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(30, 10))  # Increase figure size\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "ax = sns.boxplot(x='label', y='absolute_bbox_area', data=df, width=0.6)\n",
    "\n",
    "plt.yscale('log')  # Use log scale for y-axis\n",
    "plt.ylim(0.001)  # Set y-axis limits\n",
    "\n",
    "plt.title('Distribution of Relative Bounding Box Areas by Label', fontsize=16)\n",
    "plt.xlabel('Label', fontsize=16)\n",
    "plt.ylabel('Relative Bounding Box Area (log scale)', fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex filtering\n",
    "\n",
    "To filter for samples that have \"baseball_cap\" labels with a relative_bbox_area less than 0.003 in the ground_truth.detections, you can use [`match_labels`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.match_labels) with a combined filter. \n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "2. In the `filter` parameter, we combine two conditions:\n",
    "\n",
    "   - `F(\"label\") == \"baseball_cap\"`: This checks if the label is \"baseball_cap\"\n",
    "\n",
    "   - `F(\"relative_bbox_area\") < 0.003`: This checks if the relative_bbox_area is less than 0.003\n",
    "   \n",
    "   - We use the `&` operator to combine these conditions, ensuring both are true\n",
    "\n",
    "2. The `fields` parameter is set to `\"ground_truth.detections\"`, specifying where to apply the filter.\n",
    "\n",
    "This `match_labels` stage will select samples that have at least one detection in `ground_truth.detections` that satisfies both conditions: **it's labeled as \"baseball_cap\" and has a relative_bbox_area less than 0.003.**\n",
    "\n",
    "The resulting *filtered view* will contain only the samples from your dataset that have at least one detection labeled as \"baseball_cap\" with a relative_bbox_area less than 0.003 in the `ground_truth.detections` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_caps = F(\"label\") == \"baseball_cap\"\n",
    "\n",
    "filter_to_small_area_boxes = F(\"relative_bbox_area\") < 0.003\n",
    "\n",
    "small_baseball_cap_detection = train_dataset.match_labels(\n",
    "    filter=(filter_to_caps & filter_to_small_area_boxes), \n",
    "    fields=\"ground_truth.detections\"\n",
    "    )\n",
    "\n",
    "fo.launch_app(small_baseball_cap_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the result above has the labels we wanted to find plus all the other labels in that image. If you only want the labels that meet your requirements and nothing else, you can use [`filter_labels`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.filter_labels).\n",
    "\n",
    "\n",
    "Learn more about filtering in the following resources, as it's required reading. You will have to answer questions based on them:\n",
    "\n",
    "- [Filtering Tips and Tricks](https://medium.com/voxel51/fiftyone-filtering-tips-and-tricks-dec-09-2022-58ba13500253)\n",
    "\n",
    "- [View Stage Tips and Tricks](https://medium.com/voxel51/fiftyone-computer-vision-view-stages-tips-and-tricks-january-20-2023-fc3b7f1bb3d4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_only_small_baseball_caps = train_dataset.filter_labels(\n",
    "    filter=(filter_to_caps & filter_to_small_area_boxes),\n",
    "    field=\"ground_truth.detections\"\n",
    "    )\n",
    "\n",
    "fo.launch_app(filter_to_only_small_baseball_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify images with overlapping bounding boxes or boxes that are very close to each other.\n",
    "\n",
    "Even without predictions, [`compute_max_ious`](https://docs.voxel51.com/api/fiftyone.utils.iou.html#fiftyone.utils.iou.compute_max_ious) can be a powerful tool for exploring and understanding your dataset. Using `compute_max_ious` in this exploratory way, you're gaining valuable insights into the spatial characteristics of your dataset. This understanding is crucial for making informed decisions about data preprocessing, model selection, and potential challenges in the machine learning pipeline, even before you start training models or generating predictions. \n",
    "\n",
    "It's a data-centric approach to understanding the fundamental properties of your visual data. Here's how you can use it to gain insights into your data:\n",
    "\n",
    "1. Object Analysis:\n",
    "   - By computing IoUs between objects in the same field (e.g., ground truth), you can understand how much objects in your dataset tend to overlap.\n",
    "   - High overlap between objects can indicate complex scenes, which might be challenging for models to learn.\n",
    "   - Low overlap might suggest simpler scenes or more isolated objects.\n",
    "   - This can reveal patterns in object placement, density, or size in your images\n",
    "        - Very low IoUs might indicate many small objects, while higher IoUs could suggest larger or more prominent objects.\n",
    "   - Samples with extreme IoU values (very high or very low) might represent edge cases in your dataset that deserve special attention.\n",
    "\n",
    "2. Annotation Consistency Check:\n",
    "   - If you have multiple annotations for the same images (e.g., from different annotators), you can compare them using IoU.\n",
    "   - This helps identify discrepancies in annotation styles or potential errors.\n",
    "   - Extremely high IoUs between different objects might indicate duplicate or erroneously split annotations that need correction.   \n",
    "\n",
    "3. Potential Occlusion Detection:\n",
    "   - High IoUs between different object classes might indicate frequent occlusions in your dataset.\n",
    "   - Analyzing IoU distributions across different object classes can reveal if certain classes tend to appear in more crowded or isolated contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.iou as foui\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Compute the maximum IoU for each object in the \"ground_truth\" field\n",
    "foui.compute_max_ious(dataset, \"ground_truth\", iou_attr=\"max_iou\", classwise=True)\n",
    "\n",
    "# Filter objects with IoU greater than 0.75, which are likely duplicates\n",
    "dups_view = dataset.filter_labels(\"ground_truth\", F(\"max_iou\") > 0.75)\n",
    "\n",
    "# Visualize the duplicates\n",
    "session.view = dups_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation Density Visualization:\n",
    "Generate a visualization showing which parts of images tend to have more annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding Box Size Distribution:\n",
    "Create a scatter plot showing the distribution of bounding box sizes for different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial Distribution:\n",
    "Create heatmaps showing where objects tend to appear in images for each class.\n",
    "Analyze if certain objects are more likely to appear in specific regions of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Co-occurrence:\n",
    "Explore which objects frequently appear together in the same image.\n",
    "Create a network graph visualizing object co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label Analysis:\n",
    "For images with multiple labels, explore label co-occurrences and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Consistency Check:\n",
    "Identify instances where similar objects might be labeled inconsistently across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Filtering:\n",
    "Demonstrate complex queries combining multiple criteria (e.g., images with more than X objects of class Y, but no objects of class Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugin Ecosystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required reading:\n",
    "\n",
    "- [Using the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
