{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "# import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# train_dataset = fouh.load_from_hub(\n",
    "#     \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "#     dataset_name=\"lecture_dataset_train\", \n",
    "#     persistent=True\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because I have the dataset saved locally, I will load it like so\n",
    "train_dataset = fo.load_dataset(\"lecture_dataset_train_clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Analysis\n",
    "\n",
    "Note: that FiftyOne Detections are relative bounding box coordinates in [0, 1] in the following format:`[top-left-x, top-left-y, width, height]`\n",
    "\n",
    "Let's break down the next code block intuitively:\n",
    "\n",
    "* `rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]`\n",
    "   This calculates the relative area of a bounding box. In FiftyOne, bounding boxes are typically stored as `[x, y, width, height]` where x and y are the coordinates of the top-left corner, and width and height are relative to the image size (values between 0 and 1). So this line multiplies the relative width and height to get the relative area.\n",
    "\n",
    "* `im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")`\n",
    "   This retrieves the actual width and height of each image from the metadata. The \"$\" indicates that we're accessing a field at the sample level, not within a detection.\n",
    "\n",
    "* `abs_area = rel_bbox_area * im_width * im_height`\n",
    "   This calculates the absolute area of the bounding box in pixels. It does this by multiplying the relative area by the image dimensions.\n",
    "\n",
    "* `train_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()`\n",
    "   This adds a new field to each detection in the ground truth, storing the relative area of the bounding box.\n",
    "\n",
    "* `train_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()`\n",
    "   This adds another new field to each detection, this time storing the absolute area of the bounding box in pixels.\n",
    "\n",
    "In essence, this code is adding two new pieces of information to each detection in our dataset:\n",
    "\n",
    "1. The relative area of the bounding box (as a fraction of the total image area)\n",
    "\n",
    "2. The absolute area of the bounding box (in pixels)\n",
    "\n",
    "These new fields can be very useful for further analysis, such as filtering detections based on their size, or analyzing the distribution of object sizes in your dataset. The beauty of using `ViewField` (`F`) is that these calculations are done efficiently across the entire dataset without needing to iterate through each sample manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "train_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()\n",
    "\n",
    "train_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.first().ground_truth.detections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the upper and lower [bounds](https://docs.voxel51.com/api/fiftyone.core.aggregations.html#fiftyone.core.aggregations.Bounds) of the bounding box areas as well as other summary statistics like mean and standard deviation. Note: these are relative bounding box areas, so they represent the percentage of the total image area (hence the multiply by 100).\n",
    "\n",
    "Here's how you can do that:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = train_dataset.filter_labels(\"ground_truth\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"ground_truth.detections.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"ground_truth.detections.relative_bbox_area\")*100\n",
    "    std = view.std(\"ground_truth.detections.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of bounding box sizes across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get all labels and relative bbox areas\n",
    "labels = train_dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "bbox_areas = train_dataset.values(\"ground_truth.detections.absolute_bbox_area\")\n",
    "\n",
    "# Flatten the lists and create the dataframe\n",
    "data = []\n",
    "for sample_labels, sample_areas in zip(labels, bbox_areas):\n",
    "    for label, area in zip(sample_labels, sample_areas):\n",
    "        data.append({'label': label, 'absolute_bbox_area': area})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(30, 10))  # Increase figure size\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "ax = sns.boxplot(x='label', y='absolute_bbox_area', data=df, width=0.6)\n",
    "\n",
    "plt.yscale('log')  # Use log scale for y-axis\n",
    "plt.ylim(0.001)  # Set y-axis limits\n",
    "\n",
    "plt.title('Distribution of Absolute Bounding Box Areas by Label', fontsize=16)\n",
    "plt.xlabel('Label', fontsize=16)\n",
    "plt.ylabel('Relative Bounding Box Area (log scale)', fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex filtering\n",
    "\n",
    "To filter for samples that have \"baseball_cap\" labels with a relative_bbox_area less than 0.003 in the ground_truth.detections, you can use [`match_labels`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.match_labels) with a combined filter. \n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "2. In the `filter` parameter, we combine two conditions:\n",
    "\n",
    "   - `F(\"label\") == \"baseball_cap\"`: This checks if the label is \"baseball_cap\"\n",
    "\n",
    "   - `F(\"relative_bbox_area\") < 0.003`: This checks if the relative_bbox_area is less than 0.003\n",
    "   \n",
    "   - We use the `&` operator to combine these conditions, ensuring both are true\n",
    "\n",
    "2. The `fields` parameter is set to `\"ground_truth.detections\"`, specifying where to apply the filter.\n",
    "\n",
    "This `match_labels` stage will select samples that have at least one detection in `ground_truth.detections` that satisfies both conditions: **it's labeled as \"baseball_cap\" and has a relative_bbox_area less than 0.003.**\n",
    "\n",
    "The resulting *filtered view* will contain only the samples from your dataset that have at least one detection labeled as \"baseball_cap\" with a relative_bbox_area less than 0.003 in the `ground_truth.detections` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_caps = F(\"label\") == \"baseball_cap\"\n",
    "\n",
    "filter_to_small_area_boxes = F(\"relative_bbox_area\") < 0.003\n",
    "\n",
    "small_baseball_cap_detection = train_dataset.match_labels(\n",
    "    filter=(filter_to_caps & filter_to_small_area_boxes), \n",
    "    fields=\"ground_truth.detections\"\n",
    "    )\n",
    "\n",
    "fo.launch_app(small_baseball_cap_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the result above has the labels we wanted to find plus all the other labels in that image. If you only want the labels that meet your requirements and nothing else, you can use [`filter_labels`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.filter_labels).\n",
    "\n",
    "\n",
    "Learn more about filtering in the following resources, as it's required reading. You will have to answer questions based on them:\n",
    "\n",
    "- [Filtering Tips and Tricks](https://medium.com/voxel51/fiftyone-filtering-tips-and-tricks-dec-09-2022-58ba13500253)\n",
    "\n",
    "- [View Stage Tips and Tricks](https://medium.com/voxel51/fiftyone-computer-vision-view-stages-tips-and-tricks-january-20-2023-fc3b7f1bb3d4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_only_small_baseball_caps = train_dataset.filter_labels(\n",
    "    filter=(filter_to_caps & filter_to_small_area_boxes),\n",
    "    field=\"ground_truth.detections\"\n",
    "    )\n",
    "\n",
    "fo.launch_app(filter_to_only_small_baseball_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a visualization showing which parts of images tend to have more annotations.\n",
    "\n",
    "From a data-centric visual AI perspective, analyzing the spatial distribution of objects in your dataset is indeed important for object detection. \n",
    "\n",
    "1. Understanding dataset biases:\n",
    "   - Reveals if certain objects consistently appear in specific image regions\n",
    "   - Helps identify if the dataset is representative of real-world scenarios or if it has inherent biases\n",
    "   - Helps in diagnosing false positives or negatives in specific image regions\n",
    "   - Informs strategies for balancing the dataset not just in terms of class frequency, but also spatial distribution\n",
    "\n",
    "2. Data augmentation strategies:\n",
    "   - Informs decisions on how to augment data to improve model generalization\n",
    "   - For example, if cars are always detected in the lower half of images, you might want to augment data with cars in other positions\n",
    "\n",
    "\n",
    "However, keep in mind some limitations:\n",
    "\n",
    "1.Real-world variability: The spatial distribution in your dataset might not perfectly reflect all real-world scenarios.\n",
    "\n",
    "2. Task-specific relevance: For some object detection tasks, spatial distribution might be less critical than for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from collections import defaultdict\n",
    "\n",
    "avg_width = int(train_dataset.mean(\"metadata.width\"))\n",
    "avg_height = int(train_dataset.mean(\"metadata.height\"))\n",
    "\n",
    "# Set heatmap dimensions\n",
    "resize_factor = 2\n",
    "heatmap_width, heatmap_height = avg_width * resize_factor, avg_height * resize_factor\n",
    "\n",
    "# Initialize a dictionary to store heatmaps for each class\n",
    "class_heatmaps = defaultdict(lambda: np.zeros((heatmap_height, heatmap_width)))\n",
    "\n",
    "# Iterate through all samples and their detections\n",
    "for sample in train_dataset.iter_samples():\n",
    "    for detection in sample.ground_truth.detections:\n",
    "        x, y, w, h = detection.bounding_box\n",
    "        class_label = detection.label\n",
    "        \n",
    "        # Convert relative coordinates to pixel coordinates\n",
    "        x1, y1 = int(x * heatmap_width), int(y * heatmap_height)\n",
    "        x2, y2 = int((x + w) * heatmap_width), int((y + h) * heatmap_height)\n",
    "        \n",
    "        # Add to the class-specific heatmap\n",
    "        class_heatmaps[class_label][y1:y2, x1:x2] += 1\n",
    "\n",
    "# Apply Gaussian smoothing and normalize each heatmap\n",
    "\n",
    "smoothing_sigma = 0.25  # or any other value you choose\n",
    "for class_label in class_heatmaps:\n",
    "    class_heatmaps[class_label] = gaussian_filter(class_heatmaps[class_label], sigma=smoothing_sigma)\n",
    "    class_heatmaps[class_label] /= np.max(class_heatmaps[class_label])\n",
    "\n",
    "# Sort classes by total frequency\n",
    "sorted_classes = sorted(class_heatmaps.keys(), key=lambda x: np.sum(class_heatmaps[x]), reverse=True)\n",
    "\n",
    "# Determine grid size\n",
    "n_classes = len(sorted_classes)\n",
    "grid_size = int(np.ceil(np.sqrt(n_classes)))\n",
    "\n",
    "# Create a big plot with subplots for each class\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(4*grid_size, 4*grid_size))\n",
    "fig.suptitle(\"Spatial Distribution of Classes\", fontsize=24)\n",
    "\n",
    "for i, class_label in enumerate(sorted_classes):\n",
    "    ax = axes[i // grid_size, i % grid_size]\n",
    "    im = ax.imshow(class_heatmaps[class_label], cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(class_label)\n",
    "    ax.axis('off')\n",
    "    fig.colorbar(im, ax=ax, label='Normalized Frequency', fraction=0.046, pad=0.04)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(n_classes, grid_size * grid_size):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_classes_spatial_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Co-occurrence\n",
    "\n",
    "Explore which objects frequently appear together in the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_dict = {}\n",
    "for label in train_dataset.distinct(\"ground_truth.detections.label\"):\n",
    "    view = train_dataset.match_labels(filter = F(\"label\") == label, fields=\"ground_truth.detections\")\n",
    "    co_dict = view.count_values(\"ground_truth.detections.label\")\n",
    "    co_occurrence_dict[label] = co_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the co-occurrence dictionary to a DataFrame\n",
    "df = pd.DataFrame(co_occurrence_dict).fillna(0)\n",
    "\n",
    "# Sort the DataFrame alphabetically (both rows and columns)\n",
    "df = df.sort_index().sort_index(axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Normalize the values (optional, but can help with visualization)\n",
    "df_normalized = df.div(df.max(axis=1), axis=0)\n",
    "\n",
    "# Create a mask for the diagonal\n",
    "mask = np.eye(len(df), dtype=bool)\n",
    "\n",
    "# Create the heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                z=df_normalized,\n",
    "                x=df.columns,\n",
    "                y=df.index,\n",
    "                hoverongaps = False,\n",
    "                hovertemplate='%{y} co-occurs with %{x}: %{z:.2f}<extra></extra>'))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Object Co-occurrence Heatmap (Alphabetically Sorted)',\n",
    "    xaxis_title='Object',\n",
    "    yaxis_title='Object',\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches\n",
    "\n",
    "\n",
    "We can have an even more granular look at our data by examining [object patches](https://docs.voxel51.com/user_guide/using_views.html#object-patches). \n",
    "\n",
    "When using `to_patches`, we create a view that contains one sample per object patch in the specified field of the collection.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_view = train_dataset.to_patches(\"ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_view.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(patches_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "sunglasses_patches = patches_view.filter_labels(\"ground_truth\", F(\"label\") == \"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunglasses_patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(sunglasses_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugin Framework\n",
    "\n",
    "FiftyOne has a robust plugin ecosystem that is driven by community contributions. We'll be using Plugins throughout the course, and I'll introduce them as we need them.\n",
    "\n",
    "- [FiftyOne Plugins](https://voxel51.com/plugins/)\n",
    "\n",
    "- [Plugins on GitHub](https://github.com/voxel51/fiftyone-plugins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required reading:\n",
    "\n",
    "- [Using the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
