{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "# import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# train_dataset = fouh.load_from_hub(\n",
    "#     \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "#     dataset_name=\"lecture_dataset_train\", \n",
    "#     persistent=True\n",
    "#     )\n",
    "\n",
    "# test_dataset = fouh.load_from_hub(\n",
    "#     \"Voxel51/Coursera_lecture_dataset_test\", \n",
    "#     dataset_name=\"lecture_dataset_test\", \n",
    "#     persistent=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because I have the dataset saved locally, I will load it like so\n",
    "train_dataset = fo.load_dataset(\"lecture_dataset_train_clone\")\n",
    "\n",
    "test_dataset = fo.load_dataset(name=\"lecture_dataset_test_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.take(100)\n",
    "test_dataset = test_dataset.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first train a model. \n",
    "\n",
    "The code here isn't the star of the show, but I'll briefly describe what we're doing. After this lesson, we'll simply import this code from some helper file. This code defines a pipeline for training and evaluating a YOLO object detection model using the FiftyOne library. The main steps are:\n",
    "\n",
    "1. Export dataset to YOLO format. You can learn more about converting dataset formats [here](https://docs.voxel51.com/recipes/convert_datasets.html).\n",
    "\n",
    "2. Train YOLO model on the formatted dataset\n",
    "\n",
    "3. Run inference on evaluation set\n",
    "\n",
    "4. Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can learn more about the hypeparameters for the Ultralytics model [here](https://docs.ultralytics.com/modes/train/#train-settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will add tags for `train` and `val` to the Dataset and then converts a FiftyOne dataset to YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four.random_split(train_dataset, {\"train\": 0.90, \"val\": 0.10})\n",
    "\n",
    "train_dataset.export(\n",
    "    export_dir=\"./model_training/data\",\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    classes=train_dataset.default_classes,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "train_dataset.export(\n",
    "    export_dir=\"./model_training/data\",\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    classes=train_dataset.default_classes,\n",
    "    split='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go ahead and instantiate a model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov10m.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training hyperparameters\n",
    "\n",
    "Here are some recommendations for training YOLOv8m on images with small detections, similar looking objects, possibly mixed up labels, and a large number of detections per image:\n",
    "\n",
    "1. Image size: Use a larger input image size to help with small object detection. Consider using `imgsz=1280` or even `1536` if your GPU memory allows.\n",
    "\n",
    "2. Mosaic and scale augmentations: Enable strong mosaic and scale augmentations to help with small object detection and similar looking objects.\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16, \n",
    "               mosaic=1.0, scale=0.9)\n",
    "   ```\n",
    "\n",
    "3. Anchor optimization: YOLOv8 is anchor-free, but you can still optimize detection parameters:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               overlap_mask=True, mask_ratio=4)\n",
    "   ```\n",
    "\n",
    "4. Learning rate: Use a lower initial learning rate and cosine learning rate scheduler:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               lr0=0.001, lrf=0.01)\n",
    "   ```\n",
    "\n",
    "5. Regularization: To help with possibly mixed up labels, use label smoothing and increased weight decay:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               label_smoothing=0.1, weight_decay=0.0005)\n",
    "   ```\n",
    "\n",
    "6. Data augmentation: Use strong augmentations to help with similar looking objects:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               degrees=45, translate=0.2, scale=0.9, shear=10, \n",
    "               perspective=0.001, flipud=0.5, fliplr=0.5)\n",
    "   ```\n",
    "\n",
    "7. Focal loss: Consider using focal loss to help with class imbalance due to many detections per image:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               dfl=1.5)\n",
    "   ```\n",
    "\n",
    "8. Mixed precision training: Enable AMP for faster training:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16, amp=True)\n",
    "   ```\n",
    "\n",
    "9. Patience and epochs: Train for a longer time with patience for early stopping:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               patience=50)\n",
    "   ```\n",
    "\n",
    "### I'm just going to combine these settings into a single training config, and I'll use the same settings throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    # Dataset split\n",
    "    \"train_split\": 0.9,\n",
    "    \"val_split\": 0.1,\n",
    "\n",
    "    # Training parameters\n",
    "    \"train_params\": {\n",
    "        \"epochs\": 100,\n",
    "        \"batch\": 16,\n",
    "        \"imgsz\": 640, # just keep in mind that your gpu might not be able to handle large image sizes\n",
    "        \"lr0\": 0.001,\n",
    "        \"lrf\": 0.01,\n",
    "        \"momentum\": 0.937,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"warmup_epochs\": 3.0,\n",
    "        \"warmup_momentum\": 0.8,\n",
    "        \"warmup_bias_lr\": 0.1,\n",
    "        \"box\": 7.5,\n",
    "        \"cls\": 0.5,\n",
    "        \"dfl\": 1.5,\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"nbs\": 64,\n",
    "        \"hsv_h\": 0.015,\n",
    "        \"hsv_s\": 0.7,\n",
    "        \"hsv_v\": 0.4,\n",
    "        \"degrees\": 45,\n",
    "        \"translate\": 0.2,\n",
    "        \"scale\": 0.9,\n",
    "        \"shear\": 10,\n",
    "        \"perspective\": 0.001,\n",
    "        \"flipud\": 0.5,\n",
    "        \"fliplr\": 0.5,\n",
    "        \"mosaic\": 1.0,\n",
    "        \"mixup\": 0.1,\n",
    "        \"erasing\":0.25,\n",
    "        \"copy_paste\": 0.1,\n",
    "        \"amp\": True,\n",
    "        \"overlap_mask\": True,\n",
    "        \"mask_ratio\": 4,\n",
    "        \"patience\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data=\"./yolo_formatted/dataset.yaml\",\n",
    "    **training_config['train_params']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best trained model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = str(results.save_dir / \"weights/best.pt\")\n",
    "\n",
    "best_model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the best trained model, we can apply it to the evaluation set using the `apply_model` method of the Dataset object. Visit [the docs](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.apply_model) for more detail on the `apply_model` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_dataset.apply_model(best_model, label_field=\"baseline_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the model using the built in `evaluate_detections` method of the Dataset object. You can read more about the `evaluate_detections` method [in the docs](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.evaluate_detections), and check out [this tutorial](https://docs.voxel51.com/tutorials/evaluate_detections.html) for a different perspective on evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results = test_dataset.evaluate_detections(\n",
    "    gt_field=\"ground_truth\",  \n",
    "    pred_field=\"predictions\",\n",
    "    eval_key=f\"evalrun_baseline_predictions\",\n",
    "    compute_mAP=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with a subclass of [`DetectionResults`](https://docs.voxel51.com/api/fiftyone.core.evaluation.html#fiftyone.core.evaluation.EvaluationResults), in this case we have a [`COCODetectionResults`](https://docs.voxel51.com/api/fiftyone.utils.eval.coco.html#fiftyone.utils.eval.coco.COCODetectionResults) object.\n",
    "\n",
    "When running `evaluate_detections()` the default evaluation is COCO-style evaluation (we won't worry about other evaluation styles):\n",
    "\n",
    " - Predicted and ground truth objects are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the iou parameter\n",
    "\n",
    " - By default, only objects with the same label will be matched. Classwise matching can be disabled via the classwise parameter. Classwise means whether to only match objects with the same class label or allow matches between classes. \n",
    "\n",
    " - Ground truth objects can have an `iscrowd` attribute that indicates whether the annotation contains a crowd of objects. Multiple predictions can be matched to crowd ground truth objects. The name of this attribute can be customized by passing the optional `iscrowd` attribute of [`COCOEvaluationConfig`](https://docs.voxel51.com/api/fiftyone.utils.eval.coco.html#fiftyone.utils.eval.coco.COCOEvaluationConfig) to `evaluate_detections()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(detection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate detections\n",
    "\n",
    "Let's take a look at the new fields that have been added to our test dataset. You'll notice:\n",
    "\n",
    "- `predictions`\n",
    "\n",
    "- `evalrun_..._tp`\n",
    "\n",
    "- `evalrun_..._fp`\n",
    "\n",
    "- `evalrun_..._fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, visually inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % test_dataset.sum(\"evalrun_..._tp\"))\n",
    "print(\"FP: %d\" % test_dataset.sum(\"evalrun_..._fp\"))\n",
    "print(\"FN: %d\" % test_dataset.sum(\"evalrun_..._fn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view that has samples with the most false positives first, and only includes false positive boxes in the `predictions` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "view = (\n",
    "    test_dataset\n",
    "    .sort_by(\"evalrun_..._fp\", reverse=True)\n",
    "    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the overall mean Average Precision by calling the `mAP` method on the results object.\n",
    "\n",
    "The mAP is calculated based on  [`cocoeval`](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py)\n",
    "\n",
    "To understand how Mean Average Precision (mAP) is calculated in this codebase, let's break down the key steps:\n",
    "\n",
    "1. Evaluation preparation:\n",
    "   - The code prepares ground truth (gt) and detection (dt) data for each image and category.\n",
    "   - It computes IoU (Intersection over Union) between gt and dt objects.\n",
    "\n",
    "2. Per-image evaluation:\n",
    "   - For each image, category, area range, and max detection number:\n",
    "     - It matches detections to ground truths based on IoU thresholds.\n",
    "     - It tracks which detections match which ground truths, and which are ignored.\n",
    "\n",
    "3. Accumulation of results:\n",
    "   - It calculates precision and recall values for various IoU thresholds, categories, area ranges, and max detection numbers.\n",
    "\n",
    "4. Precision-Recall curve:\n",
    "   - For each combination of IoU threshold, category, area range, and max detection number:\n",
    "     - It sorts detections by score.\n",
    "     - It computes cumulative true positives (tp) and false positives (fp).\n",
    "     - It calculates precision and recall at each detection.\n",
    "\n",
    "5. Average Precision calculation:\n",
    "   - For each precision-recall curve:\n",
    "     - It interpolates the precision values at specific recall thresholds (0 to 1 with step 0.01).\n",
    "     - The average of these interpolated precision values gives the Average Precision (AP) for that specific setting.\n",
    "\n",
    "6. Mean Average Precision:\n",
    "   - The mAP is typically the mean of the AP values across different IoU thresholds, categories, or other dimensions, depending on the specific metric being reported.\n",
    "\n",
    "The mAP is calculated by averaging the AP values across the desired dimensions (e.g., IoU thresholds, categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a report of the results for all classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for just one class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print_report(classes = [\"jacket\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the precision-recall curves and confusion matrix like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_pr_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build on this foundation throughout the rest of the course!\n",
    "\n",
    "Required reading for this lesson is the ['Evaluating Object Detections with FiftyOne'](https://docs.voxel51.com/tutorials/evaluate_detections.html) docs page, which you can expect to have questions about in the quiz.\n",
    "\n",
    "\n",
    "If you ever need assistance, have more complex questions, or want to keep in touch, feel free to join the Voxel51 community Discord server [here](https://discord.gg/QAyfnUhfpw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
