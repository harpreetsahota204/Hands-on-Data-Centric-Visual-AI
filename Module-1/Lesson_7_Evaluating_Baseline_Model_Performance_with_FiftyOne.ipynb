{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "# import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# train_dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "# dataset_name=\"lecture_dataset_train\", \n",
    "# persistent=True)\n",
    "\n",
    "\n",
    "# test_dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_test\", \n",
    "# dataset_name=\"lecture_dataset_test\", \n",
    "# persistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because I have the dataset saved locally, I will load it like so\n",
    "# train_dataset = fo.load_dataset(\"lecture_dataset_train\")\n",
    "train_dataset = fo.load_dataset(name=\"lectrure-train-clone\")\n",
    "\n",
    "test_dataset = fo.load_dataset(name=\"lecture_dataset_test\")\n",
    "\n",
    "test_dataset = test_dataset.clone(name=\"lecture-test-clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first train a model. \n",
    "\n",
    "The code here isn't the star of the show, but I'll briefly describe what we're doing. After this lesson, we'll simply import this code from some helper file. This code defines a pipeline for training and evaluating a YOLO object detection model using the FiftyOne library. The main steps are:\n",
    "\n",
    "1. Export dataset to YOLO format\n",
    "2. Train YOLO model on the formatted dataset\n",
    "3. Run inference on evaluation set\n",
    "4. Evaluate model performance\n",
    "\n",
    "1. `export_to_yolo_format()`: Converts a FiftyOne dataset to YOLO format, handling multiple data splits if specified.\n",
    "\n",
    "2. `train_model()`: Splits the dataset, exports it to YOLO format, trains a YOLO model with the given configuration, and returns the best model.\n",
    "\n",
    "3. `run_inference_on_eval_set()`: Applies the trained model to an evaluation dataset and saves the predictions.\n",
    "\n",
    "4. `eval_model()`: Evaluates the model's performance on a dataset by computing detection metrics, including mean average precision (mAP).\n",
    "\n",
    "5. `run()`: Orchestrates the entire process by training the model, running inference on the test set, and evaluating the results.\n",
    "\n",
    "The `run()` function ties everything together, taking a training dataset, test dataset, and training configuration as inputs. It trains the model, runs inference on the test set, and returns the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_yolo_format(\n",
    "    samples,\n",
    "    classes,\n",
    "    label_field=\"ground_truth\",\n",
    "    export_dir=\"./yolo_formatted\",\n",
    "    splits=[\"train\", \"val\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Export samples to YOLO format, optionally handling multiple data splits.\n",
    "\n",
    "    Args:\n",
    "        samples (fiftyone.core.collections.SampleCollection): The dataset or samples to export.\n",
    "        export_dir (str): The directory where the exported data will be saved.\n",
    "        classes (list): A list of class names for the YOLO format.\n",
    "        label_field (str, optional): The field in the samples that contains the labels.\n",
    "            Defaults to \"ground_truth\".\n",
    "        splits (str, list, optional): The split(s) to export. Can be a single split name (str) \n",
    "            or a list of split names. If None, all samples are exported as \"val\" split. \n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    if splits is None:\n",
    "        splits = [\"val\"]\n",
    "    elif isinstance(splits, str):\n",
    "        splits = [splits]\n",
    "\n",
    "    for split in splits:\n",
    "        split_view = samples if split == \"val\" and splits == [\"val\"] else samples.match_tags(split)\n",
    "        \n",
    "        split_view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            label_field=label_field,\n",
    "            classes=classes,\n",
    "            split=split\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about converting dataset formats [here](https://docs.voxel51.com/recipes/convert_datasets.html).\n",
    "\n",
    "### Train model\n",
    "\n",
    "\n",
    "You can learn more about the hypeparameters for the Ultralytics model [here](https://docs.ultralytics.com/modes/train/#train-settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_dataset, training_config):\n",
    "    \"\"\"\n",
    "    Train the YOLO model on the given dataset using the provided configuration.\n",
    "    \"\"\"\n",
    "    four.random_split(training_dataset, {\"train\": training_config['train_split'], \"val\": training_config['val_split']})\n",
    "\n",
    "    export_to_yolo_format(\n",
    "        samples=training_dataset,\n",
    "        classes=training_dataset.default_classes,\n",
    "    )\n",
    "\n",
    "    model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "    results = model.train(\n",
    "        data=\"./yolo_formatted/dataset.yaml\",\n",
    "        **training_config['train_params']\n",
    "    )\n",
    "    \n",
    "    best_model_path = str(results.save_dir / \"weights/best.pt\")\n",
    "    best_model = YOLO(best_model_path)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some recommendations for training YOLOv8m on images with small detections, similar looking objects, possibly mixed up labels, and a large number of detections per image:\n",
    "\n",
    "1. Image size: Use a larger input image size to help with small object detection. Consider using `imgsz=1280` or even `1536` if your GPU memory allows.\n",
    "\n",
    "2. Mosaic and scale augmentations: Enable strong mosaic and scale augmentations to help with small object detection and similar looking objects.\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16, \n",
    "               mosaic=1.0, scale=0.9)\n",
    "   ```\n",
    "\n",
    "3. Anchor optimization: YOLOv8 is anchor-free, but you can still optimize detection parameters:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               overlap_mask=True, mask_ratio=4)\n",
    "   ```\n",
    "\n",
    "4. Learning rate: Use a lower initial learning rate and cosine learning rate scheduler:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               lr0=0.001, lrf=0.01)\n",
    "   ```\n",
    "\n",
    "5. Regularization: To help with possibly mixed up labels, use label smoothing and increased weight decay:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               label_smoothing=0.1, weight_decay=0.0005)\n",
    "   ```\n",
    "\n",
    "6. Data augmentation: Use strong augmentations to help with similar looking objects:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               degrees=45, translate=0.2, scale=0.9, shear=10, \n",
    "               perspective=0.001, flipud=0.5, fliplr=0.5)\n",
    "   ```\n",
    "\n",
    "7. Focal loss: Consider using focal loss to help with class imbalance due to many detections per image:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               fl_gamma=1.5)\n",
    "   ```\n",
    "\n",
    "8. Mixed precision training: Enable AMP for faster training:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16, amp=True)\n",
    "   ```\n",
    "\n",
    "9. Patience and epochs: Train for a longer time with patience for early stopping:\n",
    "\n",
    "   ```python\n",
    "   model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "               patience=50)\n",
    "   ```\n",
    "\n",
    "10. Multi-scale training: Enable multi-scale training to help with varying object sizes:\n",
    "\n",
    "    ```python\n",
    "    model.train(data=dataset.yaml, imgsz=1280, epochs=30, batch=16,\n",
    "                multi_scale=True)\n",
    "    ```\n",
    "\n",
    "### I'm just going to combine these settings into a single training config, and I'll use the same settings throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    # Dataset split\n",
    "    \"train_split\": 0.9,\n",
    "    \"val_split\": 0.1,\n",
    "\n",
    "    # Training parameters\n",
    "    \"train_params\": {\n",
    "        # \"epochs\": 30,\n",
    "        \"epochs\": 1,\n",
    "        \"batch\": 16,\n",
    "        \"imgsz\": 1280,\n",
    "        \"lr0\": 0.001,\n",
    "        \"lrf\": 0.01,\n",
    "        \"momentum\": 0.937,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"warmup_epochs\": 3.0,\n",
    "        \"warmup_momentum\": 0.8,\n",
    "        \"warmup_bias_lr\": 0.1,\n",
    "        \"box\": 7.5,\n",
    "        \"cls\": 0.5,\n",
    "        \"dfl\": 1.5,\n",
    "        \"fl_gamma\": 1.5,\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"nbs\": 64,\n",
    "        \"hsv_h\": 0.015,\n",
    "        \"hsv_s\": 0.7,\n",
    "        \"hsv_v\": 0.4,\n",
    "        \"degrees\": 45,\n",
    "        \"translate\": 0.2,\n",
    "        \"scale\": 0.9,\n",
    "        \"shear\": 10,\n",
    "        \"perspective\": 0.001,\n",
    "        \"flipud\": 0.5,\n",
    "        \"fliplr\": 0.5,\n",
    "        \"mosaic\": 1.0,\n",
    "        \"mixup\": 0.1,\n",
    "        \"copy_paste\": 0.1,\n",
    "        \"amp\": True,\n",
    "        \"multi_scale\": True,\n",
    "        \"overlap_mask\": True,\n",
    "        \"mask_ratio\": 4,\n",
    "        \"patience\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [the docs](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.apply_model) for more detail on the `apply_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_eval_set(eval_dataset, best_model):\n",
    "    \"\"\"\n",
    "    Run inference on the evaluation set using the best trained model.\n",
    "\n",
    "    Args:\n",
    "        eval_dataset (fiftyone.core.dataset.Dataset): The evaluation dataset.\n",
    "        best_model (YOLO): The best trained YOLO model.\n",
    "\n",
    "    Returns:\n",
    "        The dataset eval_dataset with predictions\n",
    "    \"\"\"\n",
    "    eval_dataset.apply_model(best_model, label_field=\"predictions\")\n",
    "    eval_dataset.save()\n",
    "    return eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about the `evaluate_detections` method [in the docs](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.evaluate_detections), and check out [this tutorial](https://docs.voxel51.com/tutorials/evaluate_detections.html) for a different perspective on evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataset_to_evaluate):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_to_evaluate (fiftyone.core.dataset.Dataset): The evaluation dataset.\n",
    "\n",
    "    Returns:\n",
    "        the mean average precision (mAP) of the model on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    detection_results = dataset_to_evaluate.evaluate_detections(\n",
    "        gt_field=\"ground_truth\",  \n",
    "        pred_field=\"predictions\",\n",
    "        eval_key=f\"evalrun_{current_datetime}\",\n",
    "        compute_mAP=True,\n",
    "        )\n",
    "\n",
    "    return detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataset, test_dataset, training_config):\n",
    "    \"\"\"\n",
    "    Main function to run the entire training and evaluation process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    best_trained_model = train_model(training_dataset=train_dataset, training_config=training_config)\n",
    "    \n",
    "    model_predictions = run_inference_on_eval_set(eval_dataset=test_dataset, best_model=best_trained_model)\n",
    "    \n",
    "    model_results = eval_model(dataset_to_evaluate=model_predictions)\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
