{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "\n",
    "# dataset = fouh.load_from_hub(\n",
    "# \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "# dataset_name=\"lecture_dataset_train\", \n",
    "# persistent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start this section by examining the labels we have in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"lecture_dataset_train\")\n",
    "\n",
    "train_dataset = dataset.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it seems our annotators may have missed some annotations...\n",
    "\n",
    "In any event, let's proceed with our investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something you may notice is the presence of classes that could potentially confuse labelers (and also, any model you may train on them).\n",
    "\n",
    "For example, human annotators (and even a model) might have trouble with:\n",
    "\n",
    "- sunglasses and goggles\n",
    "\n",
    "- coat and jacket\n",
    "\n",
    "- doughnut and pastry\n",
    "\n",
    "- baseball cap and hat\n",
    "\n",
    "Let's focus on these for see what we can learn. First, create a patches view of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_view = train_dataset.to_patches(\"ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(patches_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create filtered views containing only the labels of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "sunglasses_goggles_view = patches_view.filter_labels(field=\"ground_truth\", filter=F(\"label\").is_in([\"sunglasses\", \"goggles\"]))\n",
    "\n",
    "coat_jacket_view = patches_view.filter_labels(field=\"ground_truth\", filter=F(\"label\").is_in([\"coat\", \"jacket\"]))\n",
    "\n",
    "doughnut_pastry_view = patches_view.filter_labels(field=\"ground_truth\", filter=F(\"label\").is_in([\"doughnut\", \"pastry\"]))\n",
    "\n",
    "baseball_cap_hat_view = patches_view.filter_labels(field=\"ground_truth\", filter=F(\"label\").is_in([\"baseball_cap\", \"hat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from here we can compute embeddings for each view to see if we can glean anything about the labels. For this, let's make use of the CLIP model as it's inference is quite fast even on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from fiftyone import brain as fob\n",
    "\n",
    "sunglasses_goggles_view_results = fob.compute_visualization(\n",
    "    samples=sunglasses_goggles_view,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"sunglasses_goggles_embeddings\",\n",
    "    method=\"umap\",\n",
    "    num_dims=2,\n",
    "    num_workers=os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(sunglasses_goggles_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coat_jacket_view_results = fob.compute_visualization(\n",
    "    samples=coat_jacket_view,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"coat_jacket_embeddings\",\n",
    "    method=\"umap\",\n",
    "    num_dims=2,\n",
    "    num_workers=os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(coat_jacket_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doughnut_pastry_results = fob.compute_visualization(\n",
    "    samples=doughnut_pastry_view,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"doughnut_pastry_embeddings\",\n",
    "    method=\"umap\",\n",
    "    num_dims=2,\n",
    "    num_workers=os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(doughnut_pastry_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_cap_hat_view_results = fob.compute_visualization(\n",
    "    samples=baseball_cap_hat_view,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"baseball_cap_hat_embeddings\",\n",
    "    method=\"umap\",\n",
    "    num_dims=2,\n",
    "    num_workers=os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot detection for classifying mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import plugins\n",
    "\n",
    "plugins.download_plugin(\n",
    "    url_or_gh_repo=\"https://github.com/jacobmarks/zero-shot-prediction-plugin\"\n",
    ")\n",
    "\n",
    "plugins.install_plugin_requirements(\n",
    "    plugin_name=\"@jacobmarks/zero_shot_prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "## Access the operator via its URI (plugin name + operator name)\n",
    "zero_shot_detection_operator = foo.get_operator(\"@jacobmarks/zero_shot_prediction/zero_shot_detect\")\n",
    "\n",
    "zero_shot_detection_operator.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run zero-shot detection on all images in the dataset, specifying the labels the model to use, and the field to add the results to\n",
    "zero_shot_detection_operator(\n",
    "    train_dataset,\n",
    "    labels = ['jacket', 'coat', 'jean', 'trousers', 'short_pants', 'trash_can', 'bucket', 'flowerpot', 'helmet', 'baseball_cap', 'hat', 'sunglasses', 'goggles', 'doughnut', 'pastry', 'onion', 'tomato'],\n",
    "    model_name = \"YOLO-World\",\n",
    "    label_field = \"zero_shot_predictions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mistakenness\n",
    "\n",
    "Now we're ready to assess the mistakenness of the ground truth detections.\n",
    "\n",
    "We can do so by running the [compute_mistakenness()](https://voxel51.com/docs/fiftyone/api/fiftyone.brain.html#fiftyone.brain.compute_mistakenness) method from the FiftyOne Brain.\n",
    "\n",
    "**REMEMBER**: Since you are using model predictions to guide the mistakenness process, the better your model, the more accurate the mistakenness suggestions. Additionally, using logits of confidence scores will also provide better results. \n",
    "\n",
    "Note, you can pass `copy_missing=True` which will copy predicted objects that were deemed to be missing into the `label_field`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Compute mistakenness of annotations in `ground_truth` field using \n",
    "# predictions from `zero_shot_predictions` field as point of reference\n",
    "fob.compute_mistakenness(\n",
    "    train_dataset, \n",
    "    pred_field=\"zero_shot_predictions\", \n",
    "    label_field=\"ground_truth\",\n",
    "    copy_missing=True # you can pass this as True if you trust your model is powerful enough\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method populates a number of fields on the samples of our dataset as well as the ground truth and predicted objects:\n",
    "\n",
    "#### New ground truth object attributes (in `ground_truth` field):\n",
    "\n",
    "- `mistakenness` (float): A measure of the likelihood that a ground truth object's label is incorrect\n",
    "\n",
    "- `mistakenness_loc`: A measure of the likelihood that a ground truth object's localization (bounding box) is inaccurate\n",
    "\n",
    "- `possible_spurious`: Ground truth objects that were not matched with a predicted object and are deemed to be likely spurious annotations will have this attribute set to True\n",
    "\n",
    "#### New predicted object attributes (in `predictions` field):\n",
    "\n",
    "- `possible_missing`: If a highly confident prediction with no matching ground truth object is encountered, this attribute is set to True to indicate that it is a likely missing ground truth annotation\n",
    "\n",
    "#### Sample-level fields:\n",
    "\n",
    "- `mistakenness`: The maximum mistakenness of the ground truth objects in each sample\n",
    "\n",
    "- `possible_spurious`: The number of possible spurious ground truth objects in each sample\n",
    "\n",
    "- `possible_missing`: The number of possible missing ground truth objects in each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Sort by likelihood of mistake (most likely first)\n",
    "mistake_view = train_dataset.sort_by(\"mistakenness\", reverse=True)\n",
    "\n",
    "# Print some information about the view\n",
    "print(mistake_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some samples and detections\n",
    "# This is the first detection of the first sample\n",
    "print(mistake_view.first().ground_truth.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful query is to find all objects that have a high mistakenness, lets say > 0.95\n",
    "\n",
    "Recall that `mistakenness` measures of the likelihood that a ground truth object's label is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "highly_mistaken_view = train_dataset.filter_labels(\"ground_truth\", F(\"mistakenness\") > 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(highly_mistaken_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the results, we can see that many of these images have a bunch of predictions which actually look like they are correct, but no ground truth annotations. This is a common mistake in object detection datasets, where the annotator may have missed some objects in the image. On the other hand, there are some detections which are mislabeled...    \n",
    "\n",
    "Recall that `mistakenness_loc` is measure of the likelihood that a ground truth object's localization (bounding box) is inaccurate.\n",
    "\n",
    "We can use a similar workflow to look at objects that may be localized poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mistaken_loc_view = train_dataset.filter_labels(\"ground_truth\", F(\"mistakenness_loc\") > 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(high_mistaken_loc_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `possible_missing` field can also be useful to sort by to find instances of incorrect annotations. Similarly, `possible_spurious` can be used to find objects that the model detected that may have been missed by annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_spurious_view = train_dataset.match(F(\"possible_spurious\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(possible_spurious_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding duplicate detections\n",
    "\n",
    "When dealing with duplicate labels, there is inherent ambiguity: which one is \"correct\" and which one(s) are \"duplicate\"?\n",
    "\n",
    "By default, [find_duplicates()](https://voxel51.com/docs/fiftyone/api/fiftyone.utils.iou.html#fiftyone.utils.iou.find_duplicates) will simply iterate through the labels in each sample and flag any label whose IoU with a previous label exceeds the chosen threshold as a duplicate.\n",
    "\n",
    "Alternatively, you can pass the `method=\"greedy\"` option to instead use a greedy approach to mark the fewest number of labels as duplicate such that no non-duplicate labels have IoU greater than the specified threshold with each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.iou as foui\n",
    "\n",
    "dup_ids = foui.find_duplicates(\n",
    "    train_dataset, \n",
    "    \"ground_truth\", \n",
    "    iou_thresh=0.85, \n",
    "    classwise=True,\n",
    "    method=\"greedy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tagging and resolution\n",
    "\n",
    "In either case, it is recommended to visualize the duplicates in the App before taking any action. One convenient way to do this is to first tag the duplicates.\n",
    "\n",
    "\n",
    "Any label or collection of labels can be tagged at any time in the sample grid or expanded sample view. In the expanded sample view, individual samples can be selected by clicking on them in the media player.\n",
    "\n",
    "Labels with specific tags can then be selected with [select_labels()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html?highlight=select_labels#fiftyone.core.collections.SampleCollection.select_labels) stage and sent off to assist in improving the annotations with your annotation provided of choice. FiftyOne currently offers integrations for both [Labelbox](https://voxel51.com/docs/fiftyone/api/fiftyone.utils.labelbox.html), [Scale](https://voxel51.com/docs/fiftyone/api/fiftyone.utils.scale.html), and [CVAT](https://docs.voxel51.com/tutorials/cvat_annotation.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the automatically selected duplicates\n",
    "train_dataset.select_labels(ids=dup_ids).tag_labels(\"duplicate\")\n",
    "\n",
    "print(dataset.count_label_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use [match_labels()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match_labels) to load the samples containing at least one duplicate label in the App and use the `duplicate` tag you added to conveniently isolate and evaluate the duplicates.\n",
    "\n",
    "If you see any erroneous duplicates, simply remove the `duplicate` tag in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_view = train_dataset.match_labels(ids=dup_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dup_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you’re ready to act, you can then easily delete the duplicate labels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.delete_labels(tags=\"duplicate\")\n",
    "\n",
    "# If you want to delete every label flagged by `find_duplicates()`\n",
    "# train_dataset.delete_labels(ids=dup_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating Datasets with CVAT\n",
    "\n",
    "[FiftyOne](https://fiftyone.ai) and [CVAT](https://github.com/opencv/cvat) are two leading open-source tools, each tackling different parts of the dataset curation and improvement workflows.\n",
    "\n",
    "[The tight integration](https://voxel51.com/docs/fiftyone/integrations/cvat.html) between FiftyOne and CVAT allows you to curate and explore datasets in FiftyOne and then send off samples or existing labels for annotation in CVAT with just one line of code.\n",
    "\n",
    "\n",
    "In order to use CVAT, you must create an account on a CVAT server.\n",
    "\n",
    "By default, FiftyOne uses [app.cvat.ai](https://app.cvat.ai). So if you haven't already, go to [app.cvat.ai](https://app.cvat.ai) and create an account now.\n",
    "\n",
    "Another option is to [set up CVAT locally](https://opencv.github.io/cvat/docs/administration/basics/installation) and then [configure FiftyOne](https://voxel51.com/docs/fiftyone/integrations/cvat.html#self-hosted-servers) to use your self-hosted server. A primary benefit of setting up CVAT locally is that you are limited to 10 tasks and 500MB of data with app.cvat.ai.\n",
    "\n",
    "In any case, FiftyOne will need to connect to your CVAT account. The easiest way to configure your CVAT login credentials is to store them in environment variables.\n",
    "\n",
    "Whether you are annotating the data yourself or have a team of annotators, the workflow of uploading data from FiftyOne to CVAT is the same. The [annotate()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.annotate) method on a collection of samples lets you specify the name, type, and classes for the labels you are annotating.\n",
    "\n",
    "For example, let's annotate bounding boxes masks for the classes \"goggles\" and \"sunglasses\".\n",
    "\n",
    "We'll only include a few samples to be annotated in our view for brevity. To create annotation jobs in CVAT for these samples, we simply call [annotate()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.annotate) passing in a unique name for this annotation run and the relevant label schema information for the annotation task.\n",
    "Since we'll be annotating these samples ourselves, we pass `launch_editor=True` to automatically launch a browser window with the CVAT editor open once the data has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIFTYONE_CVAT_USERNAME\"] = getpass(\"Enter CVAT username: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIFTYONE_CVAT_PASSWORD\"] = getpass(\"Enter CVAT passoword: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A unique identifer for this run\n",
    "anno_key = \"reannotate_example\"\n",
    "\n",
    "small_sample = train_dataset.take(100, seed=51)\n",
    "\n",
    "# Upload the samples and launch CVAT\n",
    "anno_results = small_sample.annotate(\n",
    "    anno_key,\n",
    "    label_field=\"ground_truth\",\n",
    "    classes=['jacket', 'coat', 'jean', 'trousers', 'short_pants', 'trash_can', 'bucket', 'flowerpot', 'helmet', 'baseball_cap', 'hat', 'sunglasses', 'goggles', 'doughnut', 'pastry', 'onion', 'tomato'],\n",
    "    launch_editor=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample.load_annotations(anno_key, cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_results.print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth guide to the CVAT integration, check out [this notebook](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.25.0/docs/source/tutorials/cvat_annotation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging labels\n",
    "\n",
    "Depending on your usecase, it might make sense to merge labels.\n",
    "\n",
    "As we discussed before, human annotators (and even a model) might have trouble with some labels, so it might make sense to merge them like so:\n",
    "\n",
    "- sunglasses and goggles --> eyewear\n",
    "\n",
    "- coat and jacket --> outerwear\n",
    "\n",
    "- doughnut and pastry --> pastry\n",
    "\n",
    "- baseball cap and hat --> hat\n",
    "\n",
    "For that, you can use the [`map_labels`](https://docs.voxel51.com/api/fiftyone.core.view.html#fiftyone.core.view.DatasetView.map_labels) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"sunglasses\": \"eyewear\", \n",
    "    \"goggles\": \"eyewear\",\n",
    "    \"coat\":\"outerwear\",\n",
    "    \"jacket\":\"outerwear\",\n",
    "    \"baseball_cap\":\"hat\"}\n",
    "\n",
    "train_dataset = train_dataset.map_labels(\"ground_truth\", label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources:\n",
    "\n",
    "- YouTube video: [Finding and correcting mistakes](https://www.youtube.com/watch?v=WDl80g7_SBw)\n",
    "\n",
    "- Colab notebook: [Detection mistakes](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.24.1/docs/source/tutorials/detection_mistakes.ipynb)\n",
    "\n",
    "- Colab notebook: [Removing duplicate objects](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.25.0/docs/source/recipes/remove_duplicate_annos.ipynb)\n",
    "\n",
    "- Colab notebook: [Annotating datasets with CVAT](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.25.0/docs/source/tutorials/cvat_annotation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
