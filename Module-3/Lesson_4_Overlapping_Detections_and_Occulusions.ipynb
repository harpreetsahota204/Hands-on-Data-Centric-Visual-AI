{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "\n",
    "# dataset = fouh.load_from_hub(\n",
    "#     \"Voxel51/Coursera_lecture_dataset_train\", \n",
    "#     dataset_name=\"lecture_dataset_train\", \n",
    "#     persistent=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"lecture_dataset_train\")\n",
    "\n",
    "dataset = dataset.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Occlusions\n",
    "\n",
    "\n",
    "1. `detect_image_occlusion` function:\n",
    "   This function is the main driver for occlusion detection. It processes each ground truth detection in a given image sample. For each detection:\n",
    "   - It extracts a region of interest (ROI) around the bounding box, adding a 20% padding to capture potential occlusions just outside the box.\n",
    "   - It applies Gaussian blur to reduce noise, which helps in more accurate edge detection.\n",
    "   - Canny edge detection is used to identify edges in the ROI.\n",
    "   - Hough Line Transform is applied to detect straight lines from the edges.\n",
    "   - For each detected line, it checks if the line intersects or is close to the original (unpadded) bounding box using the `line_box_intersection` function.\n",
    "   - If any line is found to intersect or be close to the box, it's considered a potential occlusion.\n",
    "\n",
    "   The function includes several safety checks, such as ensuring the image can be loaded and the ROI is valid, to handle potential edge cases in the dataset.\n",
    "\n",
    "2. `line_box_intersection` function:\n",
    "   This function determines whether a given line segment potentially represents an occlusion for a bounding box. It uses two methods:\n",
    "   - Line Intersection: It checks if the line segment intersects any of the edges of the bounding box. This is done efficiently using the concept of counterclockwise (CCW) orientation of points.\n",
    "   - Proximity Check: If the line doesn't intersect the box, it checks if any part of the line is within a small threshold distance (5 pixels) from the box. This catches cases where an occluding object might be very close to but not quite touching the bounding box.\n",
    "\n",
    "   The CCW check is a clever way to determine if two line segments intersect without actually calculating the intersection point, making it computationally efficient.\n",
    "\n",
    "These functions work together to provide a heuristic approach to detecting potential occlusions. While not perfect (it may have false positives or miss some occlusions), it provides a computationally efficient way to flag images that might have occlusion issues for further review or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_image_occlusion(sample):\n",
    "    \"\"\"\n",
    "    Detect potential occlusions in bounding boxes of an image sample.\n",
    "\n",
    "    This function analyzes the areas within and around ground truth bounding boxes\n",
    "    for edge patterns that might indicate occlusions. It processes each detection\n",
    "    in the sample, applying image processing techniques to identify line patterns\n",
    "    that could represent occluding objects.\n",
    "\n",
    "    The function performs the following steps for each detection:\n",
    "    1. Extracts and pads the region of interest (ROI) around the bounding box.\n",
    "    2. Applies Gaussian blur to reduce noise in the ROI.\n",
    "    3. Uses Canny edge detection to identify edges in the ROI.\n",
    "    4. Applies Hough Line Transform to detect lines from the edges.\n",
    "    5. Checks if any detected lines intersect or are close to the original bounding box.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the sample has a 'ground_truth' field with 'detections'.\n",
    "        It also assumes that the bounding box coordinates in the detections are normalized.\n",
    "    \"\"\"\n",
    "    # Load the image from the sample's filepath\n",
    "    image = cv2.imread(sample.filepath)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not read image from {sample.filepath}\")\n",
    "        return False\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get ground truth detections from the sample\n",
    "    detections = sample.ground_truth.detections\n",
    "\n",
    "    for detection in detections:\n",
    "        # Extract bounding box coordinates and convert to absolute pixel coordinates\n",
    "        rel_x, rel_y, rel_w, rel_h = detection.bounding_box\n",
    "        x1 = int(rel_x * width)\n",
    "        y1 = int(rel_y * height)\n",
    "        x2 = int((rel_x + rel_w) * width)\n",
    "        y2 = int((rel_y + rel_h) * height)\n",
    "        \n",
    "        # Add padding around the detection (e.g., 20% of the detection size)\n",
    "        pad_x = int(0.2 * (x2 - x1))\n",
    "        pad_y = int(0.2 * (y2 - y1))\n",
    "        \n",
    "        # Ensure padded coordinates are within image bounds\n",
    "        x1_pad = max(0, x1 - pad_x)\n",
    "        y1_pad = max(0, y1 - pad_y)\n",
    "        x2_pad = min(width, x2 + pad_x)\n",
    "        y2_pad = min(height, y2 + pad_y)\n",
    "        \n",
    "        # Check if the ROI is valid\n",
    "        if x1_pad >= x2_pad or y1_pad >= y2_pad:\n",
    "            print(f\"Warning: Invalid ROI for detection in sample {sample.id}\")\n",
    "            continue\n",
    "\n",
    "        # Extract the region of interest (ROI)\n",
    "        roi = gray[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        \n",
    "        # Check if ROI is empty\n",
    "        if roi.size == 0:\n",
    "            print(f\"Warning: Empty ROI for detection in sample {sample.id}\")\n",
    "            continue\n",
    "\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(roi, (5, 5), 0)\n",
    "        \n",
    "        # Apply Canny edge detection on the ROI\n",
    "        edges = cv2.Canny(blurred, 50, 150, apertureSize=3)\n",
    "        \n",
    "        # Apply Hough Transform to detect lines in the ROI\n",
    "        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=30, minLineLength=20, maxLineGap=5)\n",
    "        \n",
    "        if lines is not None:\n",
    "            for line in lines:\n",
    "                x1_line, y1_line, x2_line, y2_line = line[0]\n",
    "                \n",
    "                # Adjust line coordinates to image coordinates\n",
    "                x1_line += x1_pad\n",
    "                x2_line += x1_pad\n",
    "                y1_line += y1_pad\n",
    "                y2_line += y1_pad\n",
    "                \n",
    "                # Check if the line intersects or is close to the original (unpadded) bounding box\n",
    "                if line_box_intersection(x1_line, y1_line, x2_line, y2_line, x1, y1, x2, y2):\n",
    "                    return True  # Potential occlusion detected\n",
    "\n",
    "    return False  # No potential occlusion detected\n",
    "\n",
    "def line_box_intersection(x1, y1, x2, y2, box_x1, box_y1, box_x2, box_y2):\n",
    "    \"\"\"\n",
    "    Check if a line intersects or is close to a bounding box.\n",
    "\n",
    "    This function uses two methods to determine if a line segment potentially\n",
    "    represents an occlusion for a given bounding box:\n",
    "\n",
    "    1. Line Intersection: It checks if the line segment intersects any of the\n",
    "       edges of the bounding box using the concept of counterclockwise orientation.\n",
    "\n",
    "    2. Proximity Check: If the line doesn't intersect, it checks if any part of\n",
    "       the line is within a small threshold distance (5 pixels) from the box.\n",
    "\n",
    "    Args:\n",
    "        x1, y1 (int): Coordinates of the start point of the line segment.\n",
    "        x2, y2 (int): Coordinates of the end point of the line segment.\n",
    "        box_x1, box_y1 (int): Coordinates of the top-left corner of the bounding box.\n",
    "        box_x2, box_y2 (int): Coordinates of the bottom-right corner of the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the line intersects or is close to the box, False otherwise.\n",
    "\n",
    "    Note:\n",
    "        The function uses a counterclockwise (CCW) orientation check to determine\n",
    "        line intersection efficiently.\n",
    "    \"\"\"\n",
    "    def ccw(A, B, C):\n",
    "        \"\"\"\n",
    "        Determine if three points are listed in counterclockwise order.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if points A, B, C are in counterclockwise order, False otherwise.\n",
    "        \"\"\"\n",
    "        return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n",
    "\n",
    "    # Check if line intersects any of the bounding box edges\n",
    "    if ccw((x1,y1),(box_x1,box_y1),(box_x2,box_y1)) != ccw((x2,y2),(box_x1,box_y1),(box_x2,box_y1)) and \\\n",
    "       ccw((x1,y1),(box_x2,box_y1),(box_x2,box_y2)) != ccw((x2,y2),(box_x2,box_y1),(box_x2,box_y2)):\n",
    "        return True\n",
    "\n",
    "    # Check if line is close to the bounding box (within 5 pixels)\n",
    "    threshold = 5\n",
    "    if (min(x1, x2) <= box_x2 + threshold and max(x1, x2) >= box_x1 - threshold and\n",
    "        min(y1, y2) <= box_y2 + threshold and max(y1, y2) >= box_y1 - threshold):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "# Process each sample in the dataset\n",
    "for sample in dataset:\n",
    "    # Detect if there's a potential occlusion in the image\n",
    "    has_occlusion = detect_image_occlusion(sample)\n",
    "    \n",
    "    # Add the occlusion flag to the sample\n",
    "    sample['has_occlusion'] = has_occlusion\n",
    "\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation techniques for occlusion\n",
    "\n",
    "The primary goal of occlusion-based data augmentation is to create models that can generalize better to real-world scenarios. In reality, objects are often partially hidden or viewed from different angles, and our models need to be prepared for these situations.\n",
    "\n",
    "This is particularly beneficial when:\n",
    "\n",
    "1. Training models to recognize objects that may be partially obscured in real-world scenarios.\n",
    "\n",
    "2. Dealing with limited training data.\n",
    "\n",
    "3. Addressing signs of overfitting to specific features in the training set.\n",
    "\n",
    "Several effective techniques can be used to simulate occlusions and improve model performance:\n",
    "\n",
    "1. **Random erase**: These methods involve randomly removing or masking sections of the input image during training.\n",
    "\n",
    "2. **CutMix**: This technique replaces a section of one image with a section from another, encouraging the model to learn from partial information and diverse contexts.\n",
    "\n",
    "3. **Mosaic**: This method stitches together four images, forcing the model to identify objects in various contexts and positions within a single training sample.\n",
    "\n",
    "4. **MixUp**: While not strictly an occlusion technique, MixUp combines multiple images at the pixel level, which can help the model learn patterns related to partially visible objects.\n",
    "\n",
    "By applying these techniques, we challenge our models to learn more robust and generalizable features, ultimately improving their performance in real-world scenarios where occlusions are common.\n",
    "\n",
    "### Ultralytics handles these augmentations for during training\n",
    "\n",
    "\n",
    "| Technique | Type | Default Value | Range | Description |\n",
    "|-----------|------|---------------|-------|-------------|\n",
    "| mixup | float | 0.0 | 0.0 - 1.0 | Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability. |\n",
    "| mosaic | float | 1.0 | 0.0 - 1.0 | Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding. |\n",
    "| erasing | float | 0.4 | 0.0 - 0.9 | Randomly erases a portion of the image during classification training, encouraging the model to focus on less obvious features for recognition. |\n",
    "| copy_paste | float | 0.0 | 0.0 - 1.0 | Copies objects from one image and pastes them onto another, useful for increasing object instances and learning object occlusion. |\n",
    "\n",
    "\n",
    "You can adjust the values for these in the training_config.yaml, or pass your chosen values directly to the `train` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing pairwise IoUs between predicted and ground truth objects\n",
    "\n",
    "You could do this using FiftyOneâ€™s `compute_ious` function which computes the pairwise IoUs between the predicted and ground truth objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.iou as foui\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "def get_overlapping_classes_view(dataset, class1, class2):\n",
    "    \"\"\"\n",
    "    Get a view of samples where bounding boxes of two specified classes overlap.\n",
    "\n",
    "    Args:\n",
    "        dataset (fiftyone.core.dataset.Dataset): The FiftyOne dataset to process.\n",
    "        class1 (str): The name of the first class.\n",
    "        class2 (str): The name of the second class.\n",
    "\n",
    "    Returns:\n",
    "        fiftyone.core.view.DatasetView: A view of the dataset containing only the samples\n",
    "        where bounding boxes of class1 and class2 overlap.\n",
    "    \"\"\"\n",
    "    # Get detections for each class\n",
    "    class1_detections = dataset.filter_labels(\"ground_truth\", F(\"label\") == class1, only_matches=False).values(\"ground_truth.detections\")\n",
    "    class2_detections = dataset.filter_labels(\"ground_truth\", F(\"label\") == class2, only_matches=False).values(\"ground_truth.detections\")\n",
    "\n",
    "    # Get sample IDs\n",
    "    sample_ids = dataset.values(\"id\")\n",
    "\n",
    "    # Find samples with overlapping detections\n",
    "    overlapping_ids = []\n",
    "    for sample_id, class1_dets, class2_dets in zip(sample_ids, class1_detections, class2_detections):\n",
    "        if class1_dets and class2_dets:\n",
    "            if foui.compute_ious(class1_dets, class2_dets).any():\n",
    "                # There exists at least one overlapping class1 and class2 detection in this sample\n",
    "                overlapping_ids.append(sample_id)\n",
    "\n",
    "    # Create and return the view\n",
    "    overlapping_view = dataset.select(overlapping_ids)\n",
    "    return overlapping_view\n",
    "\n",
    "overlapping_onion_and_tomato = get_overlapping_classes_view(dataset, \"onion\", \"tomato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(overlapping_onion_and_tomato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify images with overlapping bounding boxes or boxes that are very close to each other.\n",
    "\n",
    "Even without predictions, [`compute_max_ious`](https://docs.voxel51.com/api/fiftyone.utils.iou.html#fiftyone.utils.iou.compute_max_ious) can be a powerful tool for exploring and understanding your dataset. Using `compute_max_ious` in this exploratory way, you're gaining valuable insights into the spatial characteristics of your dataset. This understanding is crucial for making informed decisions about data preprocessing, model selection, and potential challenges in the machine learning pipeline, even before you start training models or generating predictions. \n",
    "\n",
    "It's a data-centric approach to understanding the fundamental properties of your visual data. Here's how you can use it to gain insights into your data:\n",
    "\n",
    "1. Object Analysis:\n",
    "   - By computing IoUs between objects in the same field (e.g., ground truth), you can understand how much objects in your dataset tend to overlap.\n",
    "   - High overlap between objects can indicate complex scenes, which might be challenging for models to learn.\n",
    "   - Low overlap might suggest simpler scenes or more isolated objects.\n",
    "   - This can reveal patterns in object placement, density, or size in your images\n",
    "        - Very low IoUs might indicate many small objects, while higher IoUs could suggest larger or more prominent objects.\n",
    "   - Samples with extreme IoU values (very high or very low) might represent edge cases in your dataset that deserve special attention.\n",
    "\n",
    "2. Annotation Consistency Check:\n",
    "   - If you have multiple annotations for the same images (e.g., from different annotators), you can compare them using IoU.\n",
    "   - This helps identify discrepancies in annotation styles or potential errors.\n",
    "   - Extremely high IoUs between different objects might indicate duplicate or erroneously split annotations that need correction.   \n",
    "\n",
    "3. Potential Occlusion Detection:\n",
    "   - High IoUs between different object classes might indicate frequent occlusions in your dataset.\n",
    "   - Analyzing IoU distributions across different object classes can reveal if certain classes tend to appear in more crowded or isolated contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.iou as foui\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Compute the maximum IoU for each object in the \"ground_truth\" field\n",
    "foui.compute_max_ious(dataset, \"ground_truth\", iou_attr=\"max_iou\", classwise=False)\n",
    "\n",
    "# Filter labels objects with IoU greater than 0.75, \n",
    "view_with_high_ious = dataset.filter_labels(\"ground_truth\", F(\"max_iou\") > 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in dataset.distinct(\"ground_truth.detections.label\"):\n",
    "    view = dataset.filter_labels(\"ground_truth\", F(\"label\") == label)\n",
    "    quantiles = view.quantiles(\"ground_truth.detections.max_iou\", quantiles=[0.25, 0.5, 0.75, 0.95])\n",
    "    print(f\"The quantiles of max_iou for {label} are: {quantiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(view_with_high_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more:\n",
    "\n",
    "\n",
    "- Docs: [`compute_ious`](https://docs.voxel51.com/api/fiftyone.utils.iou.html#fiftyone.utils.iou.compute_ious)\n",
    "\n",
    " - Docs: [`compute_max_ious`](https://docs.voxel51.com/api/fiftyone.utils.iou.html#fiftyone.utils.iou.compute_max_ious)\n",
    "\n",
    "\n",
    "- Colab notebook: [Augmenting datasets with Albumentations](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.25.0/docs/source/tutorials/data_augmentation.ipynb)\n",
    "\n",
    "\n",
    "If you ever need assistance, have more complex questions, or want to keep in touch, feel free to join the Voxel51 community Discord server [here](https://discord.gg/QAyfnUhfpw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
