{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U fiftyone sahi ultralytics huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# Load the dataset from Hugging Face if it's your first time using it\n",
    "\n",
    "# test_dataset = fouh.load_from_hub(\n",
    "#     \"Voxel51/Coursera_lecture_dataset_test\", \n",
    "#     dataset_name=\"lecture_dataset_test\", \n",
    "#     persistent=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"lecture_dataset_test_clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAHI\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif\">\n",
    "\n",
    "Image source: [SAHI GitHub Repo](https://github.com/obss/sahi)\n",
    "\n",
    "SAHI divides large images into smaller, overlapping slices. This makes small objects to appear relatively larger within each slice, and easier for the model to detect. The model performs detection on each slice independently, potentially capturing small objects that might be undetected in the full image.\n",
    "\n",
    "In a nutshell, here's how it works:\n",
    "\n",
    "1. SAHI takes a the full image as input.\n",
    "\n",
    "2. The input image is divided into smaller, overlapping slices. The slice size and overlap ratio are configurable parameters.\n",
    "\n",
    "3. The object detection model processes each slice independently.\n",
    "\n",
    "4. The chosen object detection model performs inference on each slice. Note: SAHI can be integrated with various object detection models, including YOLO series, without requiring modifications to the underlying detector\n",
    "\n",
    "5. The coordinates of detected objects in each slice are transformed back to the original image's coordinate system .\n",
    "\n",
    "6. Detections from all slices are collected and combined.\n",
    "\n",
    "7. Duplicate detections from overlapping slices are merged or filtered, often using non-maximum suppression (NMS).\n",
    "\n",
    "8. A consolidated list of detections for the original image is produced .\n",
    "\n",
    "Keep in mind that inference times will be longer than the original inference time. \n",
    "\n",
    "This is because we're running the model on multiple slices *per* image, which increases the number of forward passes the model has to make. This is a trade-off we're making to improve the detection of small objects.\n",
    "\n",
    "To use SAHI start by running `pip install sahi` in your terminal or notebook. \n",
    "\n",
    "Then you pass the path of your trained detection model to create an instance of SAHI's `AutoDetectionModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://huggingface.co/harpreetsahota/coursera_week1_lesson7/resolve/main/model.pt\"\n",
    "output_path = \"./model.pt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_prediction, get_sliced_prediction\n",
    "\n",
    "ckpt_path = \"model.pt\" #this will be the path to the best_model you trained in the previous module. \n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=ckpt_path,\n",
    "    confidence_threshold=0.25,\n",
    "    image_size=640,\n",
    "    # device=\"cuda\", # if you have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what the output looks like, use SAHI's `get_prediction` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_prediction(dataset.first().filepath, detection_model, verbose=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAHI results objects have a `to_fiftyone_detections()` method, which converts the results to FiftyOne detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.to_fiftyone_detections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAHI's `get_sliced_prediction()` function works in the same way as `get_prediction()`, with a few additional hyperparameters that let us configure how the image is sliced. In particular, we can specify the slice height and width, and the overlap between slices. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_result = get_sliced_prediction(\n",
    "    dataset.skip(40).first().filepath,\n",
    "    detection_model,\n",
    "    slice_height = 320,\n",
    "    slice_width = 320,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the number of detections in the sliced predictions to the number of detections in the original predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sliced_dets = len(sliced_result.to_fiftyone_detections())\n",
    "num_orig_dets = len(result.to_fiftyone_detections())\n",
    "\n",
    "print(f\"Detections predicted without slicing: {num_orig_dets}\")\n",
    "print(f\"Detections predicted with slicing: {num_sliced_dets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the change in the number of predictions.\n",
    "\n",
    "Next, we'll use [FiftyOne's Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to determine if the additional predictions are valid or just false positives.\n",
    "\n",
    "Our goal is to find optimal hyperparameters for slicing. We'll apply SAHI to the entire dataset.\n",
    "\n",
    "The function below performs sliced predictions on a sample and adds the results to a specified label field. We'll iterate over the dataset, passing each sample's filepath and slicing hyperparameters to `get_sliced_prediction()`, and then add the predictions to the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_slicing(sample, label_field, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform sliced prediction on a sample and add the results to a specified label field.\n",
    "\n",
    "    This function uses SAHI's get_sliced_prediction to perform object detection on\n",
    "    slices of the image, then converts the results to FiftyOne Detections and adds\n",
    "    them to the sample.\n",
    "\n",
    "    Args:\n",
    "        sample (fiftyone.core.sample.Sample): The FiftyOne sample to process.\n",
    "        label_field (str): The name of the field to store the predictions in.\n",
    "        **kwargs: Additional keyword arguments to pass to get_sliced_prediction.\n",
    "\n",
    "    Returns:\n",
    "        None. The function modifies the sample in-place.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that a global 'detection_model' object is available,\n",
    "        which should be an instance of a SAHI-compatible detection model.\n",
    "    \"\"\"\n",
    "    result = get_sliced_prediction(\n",
    "        sample.filepath, detection_model, verbose=0, **kwargs\n",
    "    )\n",
    "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep the slice overlap fixed at $0.2$, and see how the slice height and width affect the quality of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}\n",
    "\n",
    "small_sample = dataset.take(100, seed=51)\n",
    "\n",
    "for sample in small_sample.iter_samples(progress=True, autosave=True):\n",
    "    predict_with_slicing(sample, label_field=\"small_slices\", slice_height=320, slice_width=320, **kwargs)\n",
    "    predict_with_slicing(sample, label_field=\"large_slices\", slice_height=480, slice_width=480, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an evaluation routine comparing our predictions from each of the prediction label fields to the ground truth labels. \n",
    "\n",
    "Using the `evaluate_detections()` method will mark each detection as a true positive, false positive, or false negative. Here we use the default IoU threshold of $0.5$, but you can adjust this as needed.\n",
    "\n",
    "Note that this will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = small_sample.evaluate_detections(\"baseline_predictions\", gt_field=\"ground_truth\", eval_key=\"eval_base_model\")\n",
    "\n",
    "large_slice_results = small_sample.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\")\n",
    "\n",
    "small_slice_results = small_sample.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model results:\")\n",
    "base_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Large slice results:\")\n",
    "large_slice_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small slice results:\")\n",
    "small_slice_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as we introduce more slices, the number of false positives increases, while the number of false negatives decreases. This is expected, as the model is able to detect more objects with more slices, but also makes more mistakes! You could apply more agressive confidence thresholding to combat this increase in false positives, but even without doing this the $F_1$-score has significantly improved.\n",
    "\n",
    "### Evaluate performance on small objects \n",
    "\n",
    "Let's dive a little bit deeper into these results. We noted earlier that the model struggles with small objects, so let's see how these three approaches fare on objects smaller than $32 \\times 32$ pixels. We can perform this filtering using FiftyOne's [ViewField](https://docs.voxel51.com/recipes/creating_views.html#View-expressions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering for only small boxes\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "box_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\n",
    "rel_bbox_area = box_width * box_height\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "small_boxes_view = small_sample.filter_labels(\"ground_truth.detections\", abs_area < 32**2, only_matches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_boxes_base_results = small_boxes_view.evaluate_detections(\"baseline_predictions\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_base_model\")\n",
    "\n",
    "small_boxes_large_slice_results = small_boxes_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_large_slices\")\n",
    "\n",
    "small_boxes_small_slice_results = small_boxes_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_small_slices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Small Box — Base model results:\")\n",
    "small_boxes_base_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small Box — Large slice results:\")\n",
    "small_boxes_large_slice_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small Box — Small slice results:\")\n",
    "small_boxes_small_slice_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the value of SAHI crystal clear! The recall when using SAHI is much higher for small objects without significant dropoff in precision, leading to improved F1-score. This is especially pronounced for `` detections, where the $F_1$ score is tripled!\n",
    "\n",
    "\n",
    "### Edge cases\n",
    "Now that we know SAHI is effective at detecting small objects, let's look at the places where our predictions are most confident but do not align with the ground truth labels. We can do this by creating an evaluation patches view, filtering for predictions tagged as false positives and sorting by confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf_fp_view = small_sample.to_evaluation_patches(eval_key=\"eval_small_slices\").match(F(\"type\")==\"fp\").sort_by(\"small_slices.detection.confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(high_conf_fp_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are mostly accurate, but some ground truth labels are missing. Implementing human-in-the-loop (HITL) workflows can help correct this. We can then re-evaluate our models and train new ones with the updated data.\n",
    "\n",
    "##### Required Reading\n",
    "\n",
    "- [Albumentations Integration](https://docs.voxel51.com/integrations/albumentations.html)\n",
    "\n",
    "\n",
    "If you ever need assistance, have more complex questions, or want to keep in touch, feel free to join the Voxel51 community Discord server [here](https://discord.gg/QAyfnUhfpw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
